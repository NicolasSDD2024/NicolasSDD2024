{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Le **surapprentissage** est un probl√®me majeur dans l'entra√Ænement des r√©seaux de neurones profonds. Une technique efficace pour le combattre est le **Dropout**.\n",
    "\n",
    "Dans ce notebook, nous allons :\n",
    "1. Mais le dropout, c'est quoi ?\n",
    "2. Exemple d'application de la technique du Dropout.\n",
    "3. Comparer les performances d'un r√©seau avec et sans Dropout.\n",
    "4. G√©n√©ralisation du dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Mais le dropout, c'est quoi ?\n",
    "\n",
    "### Un peu d'histoire\n",
    "Avant l‚Äôintroduction du Dropout, plusieurs approches avaient d√©j√† explor√© l‚Äôid√©e d‚Äôajouter du bruit aux r√©seaux de neurones pour am√©liorer leur g√©n√©ralisation. **Hanson (1990)** a propos√© la **Stochastic Delta Rule**, qui injectait du bruit dans l‚Äôapprentissage des poids pour limiter le surapprentissage. **Bishop (1995)** a d√©montr√© que **perturber les entr√©es** d‚Äôun mod√®le pouvait √™tre interpr√©t√© comme une forme de r√©gularisation bay√©sienne. **LeCun et al. (1998)** ont test√© l‚Äô**ajout de bruit** dans les activations des neurones pour limiter la d√©pendance excessive aux donn√©es d'entra√Ænement. D‚Äôautres travaux, comme ceux de **Hinton & Nowlan (1992)** et **Neal (1995, 2001)**, ont √©tudi√© l‚Äôutilisation de **distributions probabilistes sur les poids et les activations** afin d‚Äôam√©liorer la g√©n√©ralisation des mod√®les.\n",
    "\n",
    "Le **Dropout** a √©t√© introduit en **2014** par **Srivastava et al.** dans leur article **\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"**. Cette technique a √©t√© d√©velopp√©e pour pallier le probl√®me du surapprentissage dans les r√©seaux de neurones profonds. Avant son introduction, les m√©thodes classiques de r√©gularisation comme la **p√©nalisation L2** et le **early stopping** √©taient couramment utilis√©es, mais elles ne suffisaient pas toujours √† √©viter l'adaptation excessive aux donn√©es d'entra√Ænement.\n",
    "\n",
    "L'id√©e principale derri√®re le Dropout √©tait inspir√©e de l'**apprentissage par ensembles**, o√π plusieurs mod√®les ind√©pendants sont combin√©s pour am√©liorer la g√©n√©ralisation. Toutefois, entra√Æner et stocker plusieurs r√©seaux de neurones profonds √©tait **co√ªteux en calcul**. Srivastava et son √©quipe ont alors cherch√© un moyen d'obtenir un effet similaire au **model averaging**, mais de mani√®re bien **plus efficace**.\n",
    "\n",
    "\n",
    "### Fonctionnement du Dropout\n",
    "\n",
    "Leur solution a √©t√© de **perturber l'apprentissage en d√©sactivant al√©atoirement des neurones √† chaque it√©ration**, for√ßant ainsi chaque neurone √† apprendre des repr√©sentations **plus robustes** sans d√©pendre excessivement de neurones sp√©cifiques. En emp√™chant la formation de **co-adaptations trop sp√©cialis√©es**, cette approche a conduit √† des mod√®les g√©n√©ralisant mieux aux donn√©es non vues. De plus, lors de la phase de **test**, tous les **neurones sont activ√©s** mais leurs **poids sont ajust√©s** pour compenser les d√©sactivations pr√©c√©dentes, simulant ainsi un moyennage implicite d'un grand nombre de **r√©seaux plus petits entra√Æn√©s en parall√®le**.\n",
    "\n",
    "\n",
    "### Impact sur l‚Äôapprentissage des r√©seaux de neurones\n",
    "Le Dropout a plusieurs effets b√©n√©fiques sur l'apprentissage :\n",
    "- **R√©duction du surapprentissage** : en emp√™chant les neurones de trop s‚Äôadapter aux donn√©es d‚Äôentra√Ænement.\n",
    "- **Am√©lioration de la robustesse** : chaque neurone doit apprendre des repr√©sentations plus g√©n√©rales, car il ne peut pas compter sur d‚Äôautres neurones sp√©cifiques.\n",
    "- **Effet d‚Äôensemble (ensemble learning)** : en √©chantillonnant diff√©rents sous-r√©seaux √† chaque it√©ration, le mod√®le final se comporte comme une combinaison de plusieurs r√©seaux diff√©rents, ce qui am√©liore la g√©n√©ralisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Exemple d'application de la technique du Dropout.\n",
    "\n",
    "### Pr√©sentation du mod√®le \n",
    "\n",
    "Ca sera plus simple pour tout le monde si je reprends un mod√®le que l'on connait. On va tout simplement utiliser le mod√®le que nous avions d√©fini au TD de deep learning sur le jeu de donn√©es FASHION-MNIST. Nous partirons de l√† et nous appliquerons la technique du dropout pour vraiment comprendre l'inter√™t de ce proc√©d√©.\n",
    "\n",
    "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) est un jeu de donn√©es contenant des images d'articles de Zalando, compos√© d'un ensemble d'entra√Ænement de 60 000 exemples et d'un ensemble de test de 10 000 exemples. Chaque exemple est une image en niveaux de gris de 28x28 pixels, associ√©e √† une √©tiquette parmi 10 classes. \n",
    "\n",
    "<img src=\"img/fashion-mnist-small.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction d'entra√Ænement avec suivi de l'historique\n",
    "def train(model, trainloader, validloader, epochs=5, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    criterion = nn.CrossEntropyLoss() # La perte est d√©finie par la Cross Entropy Loss\n",
    "    train_history, valid_history = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Mode entra√Ænement (dropout activ√©, si dropout il y a)\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Suivi de l'exactitude de l'entra√Ænement\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # Suivi de la perte et de l'exactitude de l'entra√Ænement\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        train_history.append((train_loss, train_acc))\n",
    "\n",
    "        # Validation apr√®s chaque √©poque\n",
    "        model.eval()  # Mode √©valuation (dropout d√©sactiv√©, si dropout il y a)\n",
    "        valid_loss = 0.0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validloader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "        # Suivi de la perte et de l'exactitude de la validation\n",
    "        valid_loss = valid_loss / len(validloader)\n",
    "        valid_acc = 100 * correct_valid / total_valid\n",
    "        valid_history.append((valid_loss, valid_acc))\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Epoch {epoch+1} - Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_acc:.2f}%\")\n",
    "\n",
    "    return train_history, valid_history\n",
    "\n",
    "# Visualisation des courbes de perte et de pr√©cision\n",
    "def plot_train_val(train_history, valid_history):\n",
    "    train_loss, train_acc = zip(*train_history)\n",
    "    valid_loss, valid_acc = zip(*valid_history)\n",
    "\n",
    "    # Tracer la courbe de perte\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(valid_loss, label='Valid Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Tracer la courbe de pr√©cision\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.plot(valid_acc, label='Valid Accuracy')\n",
    "    plt.title('Accuracy per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset Fashion-MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "trainset, validset = random_split(dataset, (50000, 10000))\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous choisissons de d√©finir un batch de taille 64. Nous utilisons une base de donn√©es d'entrainement de 50000 images et on validera le mod√®le sur 10000 images.\n",
    "\n",
    "Nous allons ensuite d√©finir la classe de r√©seaux de neurones denses (sans dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# D√©finition du mod√®le sans Dropout\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReLUNet\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(ReLUNet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# D√©finition du mod√®le sans Dropout\n",
    "class ReLUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        self.fc2 = nn.Linear(120,60)\n",
    "        self.fc3 = nn.Linear(60, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe est d√©finie, nous allons maintenant instancier le mod√®le, le tester et v√©rifier son erreur. Notre fonction d'entrainement/test m√©morise les scores qu'on va pouvoir comparer avec ceux du dropout plus loin dans le notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciation -> entra√Ænement -> test du mod√®le sans dropout\n",
    "model = ReLUNet()\n",
    "net_no_dropout = ReLUNet()  # Cr√©er le mod√®le sans dropout\n",
    "train_history_no_dropout, valid_history_no_dropout = train(\n",
    "    net_no_dropout, \n",
    "    trainloader, validloader, \n",
    "    epochs=5, \n",
    "    lr=0.001\n",
    "    )\n",
    "plot_train_val(train_history_no_dropout, valid_history_no_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir nettement observer ces r√©sultats il faudrait aller plus loins dans les epochs mais par soucis de temps je vous mets une capture des r√©sultats obtenus avec 50 epochs : \n",
    "\n",
    "<figure>\n",
    "    <img src=\"img/No Dropout - 50 epochs.png\" style=\"width: 900px; height: auto;\">\n",
    "    <figcaption><strong>Figure 1 :</strong> R√©sultats durant 50 epochs sans dropout (surapprentissage)</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Sans le dropout, le mod√®le a tendance √† surapprendre, nous voyons alors que lors de la validation, l'erreur diverge.\n",
    "***\n",
    "### Application du dropout\n",
    "\n",
    "Nous allons ici √©tapes par √©tapes appliquer le dropout √† notre mod√®le. Les diff√©rentes √©tapes seront pr√©sent√©es sous forme de questions auquelles vous pourrez tenter de r√©pondre. Il est fortement conseill√© de tenter de r√©soudre les questions par vous m√™me  pour comprendre le dropout en pratique et savoir l'appliquer.\n",
    "\n",
    "Torch utilise le dropout spatial, qui est d√©crit ici : https://arxiv.org/pdf/1411.4280.pdf\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 1 : Compl√®te le mod√®le suivant avec un dropout sur la premi√®re couche cach√©e. Pour commencer, tu peux poser p=0.5 (50% de chance d'√™tre d√©sactiv√©e).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/Q1.py\n",
    "class ModelWithSingleDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithSingleDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        \n",
    "        ... # Cr√©er le dropout sur cette couche √† l'aide de la fonction nn.Dropout(p)\n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        ... # Appliquer le dropout apr√®s la premi√®re couche cach√©e\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, vous avez ajout√© une couche Dropout apr√®s la premi√®re couche cach√©e. Cela permet de \"d√©sactiver\" al√©atoirement une partie des neurones de cette couche pendant l'entra√Ænement. Vous voyez que le dropout est facile √† impl√©menter dans un mod√®le. Par la suite vous allez impl√©menter le dropout sur chaque couche de notre mod√®le de classification (sauf la couche finale) et visualiser son effet et apprendre √† optimiser ses param√®tres.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 2 : Maintenant, modifie le mod√®le de classification ReLUNet en ajoutant plusieurs couches de dropout dans le r√©seau. D√©finis les dropouts comme suit : la premi√®re couche de dropout doit avoir p=0.25 et la deuxi√®me et troisi√®me soivent avoir p=0.4.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/Q2.py\n",
    "class RandomDropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        # A compl√©ter\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        # A compl√©ter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation -> entra√Ænement -> test du mod√®le avec dropouts\n",
    "net = RandomDropoutModel()\n",
    "train_history_with_dropout, valid_history_with_dropout = train(\n",
    "    net, \n",
    "    trainloader, validloader, \n",
    "    epochs=5,\n",
    "    lr=0.001,\n",
    "    )\n",
    "plot_train_val(train_history_with_dropout, valid_history_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous vous demandez pourquoi on utilise ces valeurs pr√©cises ? Dans le prochain code vous allez essayer avec d'autres valeurs, vous comprendrez. Essaye avec des petites et grandes valeurs pour bien visualiser l'effet du dropout.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 3 : Testez d'autres valeurs. Essayez avec des petites et grandes valeurs pour bien visualiser le comportement du mod√®le.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition de ton mod√®le avec Dropout\n",
    "class YourDropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YourDropoutModel, self).__init__()\n",
    "        self.dropout1 = nn.Dropout(p=...)\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        self.dropout2 = nn.Dropout(p=...)\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.dropout3 = nn.Dropout(p=...)\n",
    "        self.fc3 = nn.Linear(60, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.dropout1(x)  # Appliquer le premier dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)  # Appliquer le deuxi√®me dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout3(x)  # Appliquer le troisi√®me dropout\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation et entra√Ænement de ton mod√®le avec dropouts al√©atoires\n",
    "yournet = YourDropoutModel()\n",
    "train_history_your_dropout, valid_history_your_dropout = train(\n",
    "    yournet, \n",
    "    trainloader, validloader, \n",
    "    epochs=5,\n",
    "    lr=..., # Ici tu peux le modifier pour visualiser son effet, \n",
    "# il est conseill√© de l'augmenter l√©g√®rement par rapport au mod√®le sans dropout pour contrer le bruit g√©n√©r√©\n",
    "    earlystopping=False)\n",
    "plot_train_val(train_history_your_dropout, valid_history_your_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir nettement observer ces r√©sultats il faudrait aller plus loins dans les epochs mais par soucis de temps je vous mets une capture des r√©sultats obtenus avec 50 epochs : \n",
    "\n",
    "<figure>\n",
    "    <img src=\"img/several Dropout's config.png\" style=\"width: 900px; height: auto;\">\n",
    "    <figcaption><strong>Figure 2 :</strong> R√©sultats durant 50 epochs avec diff√©rentes configurations du dropout en fonction de leur pourcentage moyen de d√©sactivation</figcaption>\n",
    "</figure>\n",
    "\n",
    "Vous comprennez maintenant l'inter√™t de fixer des pourcentages pr√©cis, dans notre cas le r√©sultat optimal est obtenu avec p1=0.25, p2=0.4 et p3=0.4. Dans la plupart des mod√®les on √† une disposition optimale des probabilit√©s de d√©sactivation comme suit :\n",
    "- 25% de dropout dans les premi√®res couches : garde une bonne quantit√© d'informations pour apprendre des repr√©sentations g√©n√©rales des donn√©es tout en r√©duisant le risque de surapprentissage.\n",
    "- 40% de dropout dans les couches profondes : favorise la g√©n√©ralisation en for√ßant le mod√®le √† apprendre des repr√©sentations plus robustes et moins sp√©cifiques aux donn√©es d'entra√Ænement.\n",
    "\n",
    "Ainsi, la strat√©gie consiste √† augmenter le dropout dans les couches profondes, o√π le mod√®le est d√©j√† plus sp√©cialis√©, et √† le garder mod√©r√© dans les couches initiales pour ne pas perdre des informations essentielles trop t√¥t.\n",
    "\n",
    "### Bravo ! \n",
    "Vous avez appliqu√© le dropout au mod√®le de reconnaissance de chiffres de MNIST. J'esp√®re que ca s'est bien pass√© et que vous avez bien saisi comment on impl√©mente du dropout dans les diff√©rentes couches d'un r√©seau de neurones et comment on l'optimise. Nous verrons ensuite son effet sur les r√©sultats du mod√®le. \n",
    "\n",
    "Je me permets de te transmettre un petit message du cr√©ateur de ce notebook :\n",
    "\n",
    "*Salut √† toi jeune codeur ! J'esp√®re que ce petit message va te faire sourire au milieu de tes corrections de notebooks interminables. Je te souhaite bon courage pour la suite et surtout n'oublie pas de me mettre une bonne note, sinon je serais pas content. Bisous*\n",
    "***\n",
    "\n",
    "## 3. Comparer les performances d'un r√©seau avec et sans Dropout.\n",
    "\n",
    "Le code suivant permet de comparer les performances du mod√®les avec dropout \"optimis√©\" et du mod√®le sans dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er la figure avec les 2 r√©sultats pr√©c√©dents\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Tracer la courbe de perte (Loss)\n",
    "axs[0].plot([x[0] for x in train_history_no_dropout], label='Train Loss (No Dropout)', color='blue')\n",
    "axs[0].plot([x[0] for x in valid_history_no_dropout], label='Valid Loss (No Dropout)', color='red')\n",
    "axs[0].plot([x[0] for x in train_history_with_dropout], label='Train Loss (With Dropout)', color='green')\n",
    "axs[0].plot([x[0] for x in valid_history_with_dropout], label='Valid Loss (With Dropout)', color='orange')\n",
    "axs[0].plot([x[0] for x in train_history_your_dropout], label='Train Loss (Your Model)', color='yellow')\n",
    "axs[0].plot([x[0] for x in valid_history_your_dropout], label='Valid Loss (Your Model)', color='pink')\n",
    "axs[0].set_title('Perte (Loss) par √©poque')\n",
    "axs[0].set_xlabel('√âpoques')\n",
    "axs[0].set_ylabel('Perte')\n",
    "axs[0].legend()\n",
    "\n",
    "# Tracer la courbe de pr√©cision (Accuracy)\n",
    "axs[1].plot([x[1] for x in train_history_no_dropout], label='Train Accuracy (No Dropout)', color='blue')\n",
    "axs[1].plot([x[1] for x in valid_history_no_dropout], label='Valid Accuracy (No Dropout)', color='red')\n",
    "axs[1].plot([x[1] for x in train_history_with_dropout], label='Train Accuracy (With Dropout)', color='green')\n",
    "axs[1].plot([x[1] for x in valid_history_with_dropout], label='Valid Accuracy (With Dropout)', color='orange')\n",
    "axs[1].plot([x[1] for x in train_history_your_dropout], label='Train Accuracy (Your Model)', color='yellow')\n",
    "axs[1].plot([x[1] for x in valid_history_your_dropout], label='Valid Accuracy (Your Model)', color='pink')\n",
    "axs[1].set_title('Pr√©cision (Accuracy) par √©poque')\n",
    "axs[1].set_xlabel('√âpoques')\n",
    "axs[1].set_ylabel('Pr√©cision (%)')\n",
    "axs[1].legend()\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir nettement observer ces r√©sultats il faudrait aller plus loin dans les epochs mais par soucis de temps je vous mets une capture des r√©sultats obtenus avec 50 et 200 epochs : \n",
    "\n",
    "<figure>\n",
    "    <img src=\"img/resultats - p1=0,25 - p2=0,4 - p3=0,4 - 50 epochs.png\" style=\"width: 900px; height: auto;\">\n",
    "    <figcaption><strong>Figure 3 :</strong> R√©sultats durant 50 epochs avec p1=0.25, p2=0.4 et p3=0.4.</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"img/resultats - p1=0,25 - p2=0,4 - p3=0,4 - 200 epochs.png\" style=\"width: 900px; height: auto;\">\n",
    "    <figcaption><strong>Figure 4 :</strong> R√©sultats durant 200 epochs avec p1=0.25, p2=0.4 et p3=0.4.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Dans cet exemple, on peut voir que l'utilisation de dropout permet de bien limiter les effets du surapprentissage mais il vient aussi agir sur la vitesse de convergence du mod√®le. C'est donc pour cela qu'il faut choisir m√©ticuleusement les probabilit√©s de dropout pour limiter le bruit qui vient alt√©rer l'efficacit√© du mod√®le. Ici je pense que le mod√®le aurait pu encore √™tre optimiser pour obtenir une meilleure convergence de la pr√©cision en utilisant des probabilit√©s l√©g√®rement plus pr√©cises.\n",
    "\n",
    "Maintenant que vous avez vu en pratique comment s'applique et s'optimise le dropout sur un mod√®le de r√©seaux de neurones denses. Je vais vous pr√©senter comment le dropout peut s'appliquer √† d'autres architectures de r√©seaux et comment on peut le combiner √† d'autres techniques pour l'optimiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. G√©n√©ralisation du dropout\n",
    "\n",
    "### 4.1 Extension du Dropout √† d'autres mod√®les\n",
    "\n",
    "Dans la section pr√©c√©dente, nous avons appliqu√© le **dropout** √† un mod√®le de classification simple sur le dataset **Fashion-MNIST**. Cependant, cette technique de r√©gularisation ne se limite pas aux **r√©seaux de neurones denses**. \n",
    "\n",
    "Le dropout se g√©n√©ralise efficacement √† d‚Äôautres architectures. Dans les r√©seaux de neurones convolutifs (*CNNs*), il est particuli√®rement utile dans les derni√®res couches pleinement connect√©es. Les CNNs sont con√ßus pour extraire des caract√©ristiques spatiales √† travers des filtres convolutifs, et le dropout permet d‚Äôemp√™cher ces filtres de surapprendre des motifs sp√©cifiques √† l‚Äôensemble d‚Äôentra√Ænement. En d√©sactivant al√©atoirement des neurones dans les couches finales, le r√©seau est contraint d‚Äôapprendre des repr√©sentations plus g√©n√©rales, ce qui am√©liore la capacit√© de g√©n√©ralisation sur de nouvelles images.\n",
    "\n",
    "Dans les r√©seaux r√©currents (*RNNs*), le dropout est adapt√© sous la forme du *Variational Dropout*. Contrairement aux r√©seaux feedforward, les RNNs conservent une m√©moire interne qui leur permet de traiter des s√©quences. Appliquer un dropout classique d‚Äôune couche √† l‚Äôautre pourrait perturber cette m√©moire et rendre l‚Äôapprentissage instable. Le *Variational Dropout* propose une approche o√π les m√™mes neurones sont d√©sactiv√©s √† chaque pas de temps au lieu d'√™tre r√©initialis√©s √† chaque it√©ration, permettant une meilleure stabilit√© tout en pr√©servant la capacit√© d‚Äôapprentissage de longues d√©pendances.\n",
    "\n",
    "Les architectures bas√©es sur les *Transformers*, telles que *BERT* et *GPT*, utilisent √©galement le dropout pour r√©duire le surapprentissage. Dans ces mod√®les, le dropout est appliqu√© apr√®s les couches *self-attention* et *feed-forward*, ce qui emp√™che les t√©nors du mod√®le de trop se fier √† certaines unit√©s. Cette approche est essentielle pour garantir que le mod√®le g√©n√©ralise bien aux contextes qu‚Äôil n‚Äôa pas rencontr√©s pendant l‚Äôentra√Ænement.\n",
    "\n",
    "### 4.2 Combinaison du Dropout avec d‚Äôautres techniques\n",
    "\n",
    "Le dropout peut √™tre encore plus efficace lorsqu‚Äôil est combin√© √† d‚Äôautres m√©thodes de r√©gularisation. Lorsqu'il est coupl√© avec la *Batch Normalization*, les activations sont normalis√©es avant d‚Äôappliquer le dropout, ce qui stabilise l‚Äôapprentissage et acc√©l√®re la convergence. Toutefois, l‚Äôordre d‚Äôapplication est critique : appliquer le dropout avant une normalisation peut perturber la distribution des activations et r√©duire l‚Äôeffet b√©n√©fique de la normalisation.\n",
    "\n",
    "Une autre combinaison efficace est celle du dropout avec la r√©gularisation *L2* (*Weight Decay*). Alors que le dropout emp√™che la co-adaptation des neurones en masquant certaines activations, la r√©gularisation L2 impose une contrainte directe sur la magnitude des poids, √©vitant ainsi des poids excessivement grands. En combinant ces deux m√©thodes, on obtient un r√©seau plus stable et mieux g√©n√©ralis√©.\n",
    "\n",
    "Des techniques plus avanc√©es ont √©galement √©t√© d√©velopp√©es √† partir du dropout, telles que le *DropConnect*, qui ne d√©sactive pas les neurones mais certains poids du r√©seau, le *ZoneOut*, qui permet de conserver des activations inchang√©es dans les RNNs, et le *Monte Carlo Dropout*, qui applique le dropout aussi durant l‚Äôinf√©rence pour estimer la variabilit√© des pr√©dictions et produire une estimation de l‚Äôincertitude.\n",
    "\n",
    "### 4.3 Limites et Pr√©cautions d‚Äôutilisation du Dropout\n",
    "\n",
    "Bien que puissant, le dropout ne fonctionne pas toujours de mani√®re optimale. Un taux de dropout trop √©lev√© peut entra√Æner une perte excessive d‚Äôinformations et une sous-apprentissage du mod√®le, tandis qu‚Äôun taux trop faible ne produit aucun effet notable. Une calibration minutieuse est donc essentielle.\n",
    "\n",
    "L‚Äôimpact sur la convergence est √©galement un √©l√©ment √† consid√©rer. Le dropout ajoute du bruit au processus d‚Äôapprentissage, ce qui ralentit la convergence, notamment dans les r√©seaux profonds. L‚Äôutilisation de techniques comme l‚Äôajustement dynamique du *learning rate* ou l‚Äôadoption d‚Äôoptimiseurs adaptatifs tels qu‚ÄôAdam permet de compenser cet effet.\n",
    "\n",
    "Enfin, dans certains cas, le dropout n‚Äôest pas la meilleure strat√©gie de r√©gularisation. Lorsque les donn√©es d‚Äôentra√Ænement sont limit√©es, des m√©thodes comme la *data augmentation* peuvent s‚Äôav√©rer plus efficaces. De m√™me, certaines architectures modernes comme les *ResNets* b√©n√©ficient davantage des connexions r√©siduelles que du dropout pour assurer une meilleure g√©n√©ralisation.\n",
    "\n",
    "***\n",
    "## Conclusion\n",
    "Le dropout est une m√©thode de r√©gularisation tr√®s efficace qui peut √™tre g√©n√©ralis√©e √† divers types de r√©seaux neuronaux et combin√©e avec d‚Äôautres techniques pour am√©liorer la g√©n√©ralisation. Le dropout est assez facile √† impl√©menter car Pytorch propose des fonctions tr√®s intuitives pour appliquer le dropout. Cependant, il doit √™tre ajust√© correctement pour √©viter une perte excessive d‚Äôinformation ou un ralentissement de l‚Äôapprentissage. Il est donc essentiel de tester diff√©rentes configurations et de l‚Äôadapter au contexte du mod√®le utilis√©.\n",
    "\n",
    "***\n",
    "## R√©f√©rences\n",
    "1. <a href=\"references/Starting paper.pdf\" target=\"_blank\"> *Dropout: A simple way to prevent neural networks from overfitting*</a> (2014). Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R.\n",
    "2. <a href=\"references/Starting paper.pdf\" target=\"_blank\"> *Efficient Object Localization Using Convolutional Networks*</a> (2014). Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, Christoph Bregler, New York University. \n",
    "3. <a href=\"https://openai.com/index/chatgpt/\" target=\"_blank\">üê± Mon ami le chat</a>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDD_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
