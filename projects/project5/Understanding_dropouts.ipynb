{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Le **surapprentissage** est un problème majeur dans l'entraînement des réseaux de neurones profonds. Une technique efficace pour le combattre est le **Dropout**.\n",
    "\n",
    "Dans ce notebook, nous allons :\n",
    "1. Mais le dropout, c'est quoi ?\n",
    "2. Exemple d'application de la technique du Dropout.\n",
    "3. Comparer les performances d'un réseau avec et sans Dropout.\n",
    "4. Généralisation du dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Mais le dropout, c'est quoi ?\n",
    "\n",
    "### Un peu d'histoire\n",
    "Avant l’introduction du Dropout, plusieurs approches avaient déjà exploré l’idée d’ajouter du bruit aux réseaux de neurones pour améliorer leur généralisation. **Hanson (1990)** a proposé la **Stochastic Delta Rule**, qui injectait du bruit dans l’apprentissage des poids pour limiter le surapprentissage. **Bishop (1995)** a démontré que **perturber les entrées** d’un modèle pouvait être interprété comme une forme de régularisation bayésienne. **LeCun et al. (1998)** ont testé l’**ajout de bruit** dans les activations des neurones pour limiter la dépendance excessive aux données d'entraînement. D’autres travaux, comme ceux de **Hinton & Nowlan (1992)** et **Neal (1995, 2001)**, ont étudié l’utilisation de **distributions probabilistes sur les poids et les activations** afin d’améliorer la généralisation des modèles.\n",
    "\n",
    "Le **Dropout** a été introduit en **2014** par **Srivastava et al.** dans leur article **\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"**. Cette technique a été développée pour pallier le problème du surapprentissage dans les réseaux de neurones profonds. Avant son introduction, les méthodes classiques de régularisation comme la **pénalisation L2** et le **early stopping** étaient couramment utilisées, mais elles ne suffisaient pas toujours à éviter l'adaptation excessive aux données d'entraînement.\n",
    "\n",
    "L'idée principale derrière le Dropout était inspirée de l'**apprentissage par ensembles**, où plusieurs modèles indépendants sont combinés pour améliorer la généralisation. Toutefois, entraîner et stocker plusieurs réseaux de neurones profonds était **coûteux en calcul**. Srivastava et son équipe ont alors cherché un moyen d'obtenir un effet similaire au **model averaging**, mais de manière bien **plus efficace**.\n",
    "\n",
    "\n",
    "### Fonctionnement du Dropout\n",
    "\n",
    "Leur solution a été de **perturber l'apprentissage en désactivant aléatoirement des neurones à chaque itération**, forçant ainsi chaque neurone à apprendre des représentations **plus robustes** sans dépendre excessivement de neurones spécifiques. En empêchant la formation de **co-adaptations trop spécialisées**, cette approche a conduit à des modèles généralisant mieux aux données non vues. De plus, lors de la phase de **test**, tous les **neurones sont activés** mais leurs **poids sont ajustés** pour compenser les désactivations précédentes, simulant ainsi un moyennage implicite d'un grand nombre de **réseaux plus petits entraînés en parallèle**.\n",
    "\n",
    "\n",
    "### Impact sur l’apprentissage des réseaux de neurones\n",
    "Le Dropout a plusieurs effets bénéfiques sur l'apprentissage :\n",
    "- **Réduction du surapprentissage** : en empêchant les neurones de trop s’adapter aux données d’entraînement.\n",
    "- **Amélioration de la robustesse** : chaque neurone doit apprendre des représentations plus générales, car il ne peut pas compter sur d’autres neurones spécifiques.\n",
    "- **Effet d’ensemble (ensemble learning)** : en échantillonnant différents sous-réseaux à chaque itération, le modèle final se comporte comme une combinaison de plusieurs réseaux différents, ce qui améliore la généralisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Exemple d'application de la technique du Dropout.\n",
    "\n",
    "### Présentation du modèle \n",
    "\n",
    "Ca sera plus simple pour tout le monde si je reprends un modèle que l'on connait. On va tout simplement utiliser le modèle que nous avions défini au TD de deep learning sur le jeu de données FASHION-MNIST. Nous partirons de là et nous appliquerons la technique du dropout pour vraiment comprendre l'interêt de ce procédé.\n",
    "\n",
    "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) est un jeu de données contenant des images d'articles de Zalando, composé d'un ensemble d'entraînement de 60 000 exemples et d'un ensemble de test de 10 000 exemples. Chaque exemple est une image en niveaux de gris de 28x28 pixels, associée à une étiquette parmi 10 classes. \n",
    "\n",
    "<img src=\"img/fashion-mnist-small.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction d'entraînement avec suivi de l'historique\n",
    "def train(model, trainloader, validloader, epochs=5, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    criterion = nn.CrossEntropyLoss() # La perte est définie par la Cross Entropy Loss\n",
    "    train_history, valid_history = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Mode entraînement (dropout activé, si dropout il y a)\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Suivi de l'exactitude de l'entraînement\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # Suivi de la perte et de l'exactitude de l'entraînement\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        train_history.append((train_loss, train_acc))\n",
    "\n",
    "        # Validation après chaque époque\n",
    "        model.eval()  # Mode évaluation (dropout désactivé, si dropout il y a)\n",
    "        valid_loss = 0.0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validloader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "        # Suivi de la perte et de l'exactitude de la validation\n",
    "        valid_loss = valid_loss / len(validloader)\n",
    "        valid_acc = 100 * correct_valid / total_valid\n",
    "        valid_history.append((valid_loss, valid_acc))\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Epoch {epoch+1} - Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_acc:.2f}%\")\n",
    "\n",
    "    return train_history, valid_history\n",
    "\n",
    "# Visualisation des courbes de perte et de précision\n",
    "def plot_train_val(train_history, valid_history):\n",
    "    train_loss, train_acc = zip(*train_history)\n",
    "    valid_loss, valid_acc = zip(*valid_history)\n",
    "\n",
    "    # Tracer la courbe de perte\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(valid_loss, label='Valid Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Tracer la courbe de précision\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.plot(valid_acc, label='Valid Accuracy')\n",
    "    plt.title('Accuracy per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset Fashion-MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "trainset, validset = random_split(dataset, (50000, 10000))\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous choisissons de définir un batch de taille 64. Nous utilisons une base de données d'entrainement de 50000 images et on validera le modèle sur 10000 images.\n",
    "\n",
    "Nous allons ensuite définir la classe de réseaux de neurones denses (sans dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Définition du modèle sans Dropout\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReLUNet\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(ReLUNet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Définition du modèle sans Dropout\n",
    "class ReLUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        self.fc2 = nn.Linear(120,60)\n",
    "        self.fc3 = nn.Linear(60, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe est définie, nous allons maintenant instancier le modèle, le tester et vérifier son erreur. Notre fonction d'entrainement/test mémorise les scores qu'on va pouvoir comparer avec ceux du dropout plus loin dans le notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciation -> entraînement -> test du modèle sans dropout\n",
    "model = ReLUNet()\n",
    "net_no_dropout = ReLUNet()  # Créer le modèle sans dropout\n",
    "train_history_no_dropout, valid_history_no_dropout = train(\n",
    "    net_no_dropout, \n",
    "    trainloader, validloader, \n",
    "    epochs=5, \n",
    "    lr=0.001\n",
    "    )\n",
    "plot_train_val(train_history_no_dropout, valid_history_no_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir nettement observer ces résultats il faudrait aller plus loins dans les epochs mais par soucis de temps je vous mets une capture des résultats obtenus avec 50 epochs : \n",
    "\n",
    "<figure>\n",
    "    <img src=\"img/No Dropout - 50 epochs.png\" style=\"width: 900px; height: auto;\">\n",
    "    <figcaption><strong>Figure 1 :</strong> Résultats durant 50 epochs sans dropout (surapprentissage)</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Sans le dropout, le modèle a tendance à surapprendre, nous voyons alors que lors de la validation, l'erreur diverge.\n",
    "***\n",
    "### Application du dropout\n",
    "\n",
    "Nous allons ici étapes par étapes appliquer le dropout à notre modèle. Les différentes étapes seront présentées sous forme de questions auquelles vous pourrez tenter de répondre. Il est fortement conseillé de tenter de résoudre les questions par vous même  pour comprendre le dropout en pratique et savoir l'appliquer.\n",
    "\n",
    "Torch utilise le dropout spatial, qui est décrit ici : https://arxiv.org/pdf/1411.4280.pdf\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 1 : Complète le modèle suivant avec un dropout sur la première couche cachée. Pour commencer, tu peux poser p=0.5 (50% de chance d'être désactivée).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/Q1.py\n",
    "class ModelWithSingleDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithSingleDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        \n",
    "        ... # Créer le dropout sur cette couche à l'aide de la fonction nn.Dropout(p)\n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        ... # Appliquer le dropout après la première couche cachée\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, vous avez ajouté une couche Dropout après la première couche cachée. Cela permet de \"désactiver\" aléatoirement une partie des neurones de cette couche pendant l'entraînement. Vous voyez que le dropout est facile à implémenter dans un modèle. Par la suite vous allez implémenter le dropout sur chaque couche de notre modèle de classification (sauf la couche finale) et visualiser son effet et apprendre à optimiser ses paramètres.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 2 : Maintenant, modifie le modèle de classification ReLUNet en ajoutant plusieurs couches de dropout dans le réseau. Définis les dropouts comme suit : la première couche de dropout doit avoir p=0.25 et la deuxième et troisième soivent avoir p=0.4.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/Q2.py\n",
    "class RandomDropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        # A compléter\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        # A compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation -> entraînement -> test du modèle avec dropouts\n",
    "net = RandomDropoutModel()\n",
    "train_history_with_dropout, valid_history_with_dropout = train(\n",
    "    net, \n",
    "    trainloader, validloader, \n",
    "    epochs=5,\n",
    "    lr=0.001,\n",
    "    )\n",
    "plot_train_val(train_history_with_dropout, valid_history_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous vous demandez pourquoi on utilise ces valeurs précises ? Dans le prochain code vous allez essayer avec d'autres valeurs, vous comprendrez. Essaye avec des petites et grandes valeurs pour bien visualiser l'effet du dropout.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 3 : Testez d'autres valeurs. Essayez avec des petites et grandes valeurs pour bien visualiser le comportement du modèle.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de ton modèle avec Dropout\n",
    "class YourDropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YourDropoutModel, self).__init__()\n",
    "        self.dropout1 = nn.Dropout(p=...)\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        self.dropout2 = nn.Dropout(p=...)\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.dropout3 = nn.Dropout(p=...)\n",
    "        self.fc3 = nn.Linear(60, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.dropout1(x)  # Appliquer le premier dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)  # Appliquer le deuxième dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout3(x)  # Appliquer le troisième dropout\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation et entraînement de ton modèle avec dropouts aléatoires\n",
    "yournet = YourDropoutModel()\n",
    "train_history_your_dropout, valid_history_your_dropout = train(\n",
    "    yournet, \n",
    "    trainloader, validloader, \n",
    "    epochs=5,\n",
    "    lr=..., # Ici tu peux le modifier pour visualiser son effet, \n",
    "# il est conseillé de l'augmenter légèrement par rapport au modèle sans dropout pour contrer le bruit généré\n",
    "    earlystopping=False)\n",
    "plot_train_val(train_history_your_dropout, valid_history_your_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir nettement observer ces résultats il faudrait aller plus loins dans les epochs mais par soucis de temps je vous mets une capture des résultats obtenus avec 50 epochs : \n",
    "\n",
    "<figure>\n",
    "    <img src=\"img/several Dropout's config.png\" style=\"width: 900px; height: auto;\">\n",
    "    <figcaption><strong>Figure 2 :</strong> Résultats durant 50 epochs avec différentes configurations du dropout en fonction de leur pourcentage moyen de désactivation</figcaption>\n",
    "</figure>\n",
    "\n",
    "Vous comprennez maintenant l'interêt de fixer des pourcentages précis, dans notre cas le résultat optimal est obtenu avec p1=0.25, p2=0.4 et p3=0.4. Dans la plupart des modèles on à une disposition optimale des probabilités de désactivation comme suit :\n",
    "- 25% de dropout dans les premières couches : garde une bonne quantité d'informations pour apprendre des représentations générales des données tout en réduisant le risque de surapprentissage.\n",
    "- 40% de dropout dans les couches profondes : favorise la généralisation en forçant le modèle à apprendre des représentations plus robustes et moins spécifiques aux données d'entraînement.\n",
    "\n",
    "Ainsi, la stratégie consiste à augmenter le dropout dans les couches profondes, où le modèle est déjà plus spécialisé, et à le garder modéré dans les couches initiales pour ne pas perdre des informations essentielles trop tôt.\n",
    "\n",
    "### Bravo ! \n",
    "Vous avez appliqué le dropout au modèle de reconnaissance de chiffres de MNIST. J'espère que ca s'est bien passé et que vous avez bien saisi comment on implémente du dropout dans les différentes couches d'un réseau de neurones et comment on l'optimise. Nous verrons ensuite son effet sur les résultats du modèle. \n",
    "\n",
    "Je me permets de te transmettre un petit message du créateur de ce notebook :\n",
    "\n",
    "*Salut à toi jeune codeur ! J'espère que ce petit message va te faire sourire au milieu de tes corrections de notebooks interminables. Je te souhaite bon courage pour la suite et surtout n'oublie pas de me mettre une bonne note, sinon je serais pas content. Bisous*\n",
    "***\n",
    "\n",
    "## 3. Comparer les performances d'un réseau avec et sans Dropout.\n",
    "\n",
    "Le code suivant permet de comparer les performances du modèles avec dropout \"optimisé\" et du modèle sans dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la figure avec les 2 résultats précédents\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Tracer la courbe de perte (Loss)\n",
    "axs[0].plot([x[0] for x in train_history_no_dropout], label='Train Loss (No Dropout)', color='blue')\n",
    "axs[0].plot([x[0] for x in valid_history_no_dropout], label='Valid Loss (No Dropout)', color='red')\n",
    "axs[0].plot([x[0] for x in train_history_with_dropout], label='Train Loss (With Dropout)', color='green')\n",
    "axs[0].plot([x[0] for x in valid_history_with_dropout], label='Valid Loss (With Dropout)', color='orange')\n",
    "axs[0].plot([x[0] for x in train_history_your_dropout], label='Train Loss (Your Model)', color='yellow')\n",
    "axs[0].plot([x[0] for x in valid_history_your_dropout], label='Valid Loss (Your Model)', color='pink')\n",
    "axs[0].set_title('Perte (Loss) par époque')\n",
    "axs[0].set_xlabel('Époques')\n",
    "axs[0].set_ylabel('Perte')\n",
    "axs[0].legend()\n",
    "\n",
    "# Tracer la courbe de précision (Accuracy)\n",
    "axs[1].plot([x[1] for x in train_history_no_dropout], label='Train Accuracy (No Dropout)', color='blue')\n",
    "axs[1].plot([x[1] for x in valid_history_no_dropout], label='Valid Accuracy (No Dropout)', color='red')\n",
    "axs[1].plot([x[1] for x in train_history_with_dropout], label='Train Accuracy (With Dropout)', color='green')\n",
    "axs[1].plot([x[1] for x in valid_history_with_dropout], label='Valid Accuracy (With Dropout)', color='orange')\n",
    "axs[1].plot([x[1] for x in train_history_your_dropout], label='Train Accuracy (Your Model)', color='yellow')\n",
    "axs[1].plot([x[1] for x in valid_history_your_dropout], label='Valid Accuracy (Your Model)', color='pink')\n",
    "axs[1].set_title('Précision (Accuracy) par époque')\n",
    "axs[1].set_xlabel('Époques')\n",
    "axs[1].set_ylabel('Précision (%)')\n",
    "axs[1].legend()\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir nettement observer ces résultats il faudrait aller plus loin dans les epochs mais par soucis de temps je vous mets une capture des résultats obtenus avec 50 et 200 epochs : \n",
    "\n",
    "<figure>\n",
    "    <img src=\"img/resultats - p1=0,25 - p2=0,4 - p3=0,4 - 50 epochs.png\" style=\"width: 900px; height: auto;\">\n",
    "    <figcaption><strong>Figure 3 :</strong> Résultats durant 50 epochs avec p1=0.25, p2=0.4 et p3=0.4.</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"img/resultats - p1=0,25 - p2=0,4 - p3=0,4 - 200 epochs.png\" style=\"width: 900px; height: auto;\">\n",
    "    <figcaption><strong>Figure 4 :</strong> Résultats durant 200 epochs avec p1=0.25, p2=0.4 et p3=0.4.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Dans cet exemple, on peut voir que l'utilisation de dropout permet de bien limiter les effets du surapprentissage mais il vient aussi agir sur la vitesse de convergence du modèle. C'est donc pour cela qu'il faut choisir méticuleusement les probabilités de dropout pour limiter le bruit qui vient altérer l'efficacité du modèle. Ici je pense que le modèle aurait pu encore être optimiser pour obtenir une meilleure convergence de la précision en utilisant des probabilités légèrement plus précises.\n",
    "\n",
    "Maintenant que vous avez vu en pratique comment s'applique et s'optimise le dropout sur un modèle de réseaux de neurones denses. Je vais vous présenter comment le dropout peut s'appliquer à d'autres architectures de réseaux et comment on peut le combiner à d'autres techniques pour l'optimiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Généralisation du dropout\n",
    "\n",
    "### 4.1 Extension du Dropout à d'autres modèles\n",
    "\n",
    "Dans la section précédente, nous avons appliqué le **dropout** à un modèle de classification simple sur le dataset **Fashion-MNIST**. Cependant, cette technique de régularisation ne se limite pas aux **réseaux de neurones denses**. \n",
    "\n",
    "Le dropout se généralise efficacement à d’autres architectures. Dans les réseaux de neurones convolutifs (*CNNs*), il est particulièrement utile dans les dernières couches pleinement connectées. Les CNNs sont conçus pour extraire des caractéristiques spatiales à travers des filtres convolutifs, et le dropout permet d’empêcher ces filtres de surapprendre des motifs spécifiques à l’ensemble d’entraînement. En désactivant aléatoirement des neurones dans les couches finales, le réseau est contraint d’apprendre des représentations plus générales, ce qui améliore la capacité de généralisation sur de nouvelles images.\n",
    "\n",
    "Dans les réseaux récurrents (*RNNs*), le dropout est adapté sous la forme du *Variational Dropout*. Contrairement aux réseaux feedforward, les RNNs conservent une mémoire interne qui leur permet de traiter des séquences. Appliquer un dropout classique d’une couche à l’autre pourrait perturber cette mémoire et rendre l’apprentissage instable. Le *Variational Dropout* propose une approche où les mêmes neurones sont désactivés à chaque pas de temps au lieu d'être réinitialisés à chaque itération, permettant une meilleure stabilité tout en préservant la capacité d’apprentissage de longues dépendances.\n",
    "\n",
    "Les architectures basées sur les *Transformers*, telles que *BERT* et *GPT*, utilisent également le dropout pour réduire le surapprentissage. Dans ces modèles, le dropout est appliqué après les couches *self-attention* et *feed-forward*, ce qui empêche les ténors du modèle de trop se fier à certaines unités. Cette approche est essentielle pour garantir que le modèle généralise bien aux contextes qu’il n’a pas rencontrés pendant l’entraînement.\n",
    "\n",
    "### 4.2 Combinaison du Dropout avec d’autres techniques\n",
    "\n",
    "Le dropout peut être encore plus efficace lorsqu’il est combiné à d’autres méthodes de régularisation. Lorsqu'il est couplé avec la *Batch Normalization*, les activations sont normalisées avant d’appliquer le dropout, ce qui stabilise l’apprentissage et accélère la convergence. Toutefois, l’ordre d’application est critique : appliquer le dropout avant une normalisation peut perturber la distribution des activations et réduire l’effet bénéfique de la normalisation.\n",
    "\n",
    "Une autre combinaison efficace est celle du dropout avec la régularisation *L2* (*Weight Decay*). Alors que le dropout empêche la co-adaptation des neurones en masquant certaines activations, la régularisation L2 impose une contrainte directe sur la magnitude des poids, évitant ainsi des poids excessivement grands. En combinant ces deux méthodes, on obtient un réseau plus stable et mieux généralisé.\n",
    "\n",
    "Des techniques plus avancées ont également été développées à partir du dropout, telles que le *DropConnect*, qui ne désactive pas les neurones mais certains poids du réseau, le *ZoneOut*, qui permet de conserver des activations inchangées dans les RNNs, et le *Monte Carlo Dropout*, qui applique le dropout aussi durant l’inférence pour estimer la variabilité des prédictions et produire une estimation de l’incertitude.\n",
    "\n",
    "### 4.3 Limites et Précautions d’utilisation du Dropout\n",
    "\n",
    "Bien que puissant, le dropout ne fonctionne pas toujours de manière optimale. Un taux de dropout trop élevé peut entraîner une perte excessive d’informations et une sous-apprentissage du modèle, tandis qu’un taux trop faible ne produit aucun effet notable. Une calibration minutieuse est donc essentielle.\n",
    "\n",
    "L’impact sur la convergence est également un élément à considérer. Le dropout ajoute du bruit au processus d’apprentissage, ce qui ralentit la convergence, notamment dans les réseaux profonds. L’utilisation de techniques comme l’ajustement dynamique du *learning rate* ou l’adoption d’optimiseurs adaptatifs tels qu’Adam permet de compenser cet effet.\n",
    "\n",
    "Enfin, dans certains cas, le dropout n’est pas la meilleure stratégie de régularisation. Lorsque les données d’entraînement sont limitées, des méthodes comme la *data augmentation* peuvent s’avérer plus efficaces. De même, certaines architectures modernes comme les *ResNets* bénéficient davantage des connexions résiduelles que du dropout pour assurer une meilleure généralisation.\n",
    "\n",
    "***\n",
    "## Conclusion\n",
    "Le dropout est une méthode de régularisation très efficace qui peut être généralisée à divers types de réseaux neuronaux et combinée avec d’autres techniques pour améliorer la généralisation. Le dropout est assez facile à implémenter car Pytorch propose des fonctions très intuitives pour appliquer le dropout. Cependant, il doit être ajusté correctement pour éviter une perte excessive d’information ou un ralentissement de l’apprentissage. Il est donc essentiel de tester différentes configurations et de l’adapter au contexte du modèle utilisé.\n",
    "\n",
    "***\n",
    "## Références\n",
    "1. <a href=\"references/Starting paper.pdf\" target=\"_blank\"> *Dropout: A simple way to prevent neural networks from overfitting*</a> (2014). Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R.\n",
    "2. <a href=\"references/Starting paper.pdf\" target=\"_blank\"> *Efficient Object Localization Using Convolutional Networks*</a> (2014). Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, Christoph Bregler, New York University. \n",
    "3. <a href=\"https://openai.com/index/chatgpt/\" target=\"_blank\">🐱 Mon ami le chat</a>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDD_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
