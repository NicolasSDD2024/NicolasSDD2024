{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Le **surapprentissage** est un problème majeur dans l'entraînement des réseaux de neurones profonds. Une technique efficace pour le combattre est le **Dropout**.\n",
    "\n",
    "Dans ce notebook, nous allons :\n",
    "1. Mais le dropout, c'est quoi ?\n",
    "2. Exemple d'application de la technique du Dropout.\n",
    "3. Comparer les performances d'un réseau avec et sans Dropout.\n",
    "4. Présenter cette technique sur d'autres types de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: black; padding: 10px; border-radius: 5px;\">\n",
    "Plan d'action, on va faire un exemple de dropout et ensuite on va venir comparer aux autres techniques de régularisation et enfin on va venir les combiner et montrer que le dropout se combine très bien avec du L2 ou norma par lots par exemple dans notre cas (MNIST).\n",
    "\n",
    "Ensuite on va venir faire un état de l'art des différents réseaux de neuronnes qui sont efficaces avec le dropouts ou pas et conclure quant à cette technique et son utilisation sur les modèles de deep.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Mais le dropout, c'est quoi ?\n",
    "\n",
    "### Un peu d'histoire\n",
    "Avant l’introduction du Dropout, plusieurs approches avaient déjà exploré l’idée d’ajouter du bruit aux réseaux de neurones pour améliorer leur généralisation. **Hanson (1990)** a proposé la **Stochastic Delta Rule**, qui injectait du bruit dans l’apprentissage des poids pour limiter le surapprentissage. **Bishop (1995)** a démontré que **perturber les entrées** d’un modèle pouvait être interprété comme une forme de régularisation bayésienne. **LeCun et al. (1998)** ont testé l’**ajout de bruit** dans les activations des neurones pour limiter la dépendance excessive aux données d'entraînement. D’autres travaux, comme ceux de **Hinton & Nowlan (1992)** et **Neal (1995, 2001)**, ont étudié l’utilisation de **distributions probabilistes sur les poids et les activations** afin d’améliorer la généralisation des modèles.\n",
    "\n",
    "Le **Dropout** a été introduit en **2014** par **Srivastava et al.** dans leur article **\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"**. Cette technique a été développée pour pallier le problème du surapprentissage dans les réseaux de neurones profonds. Avant son introduction, les méthodes classiques de régularisation comme la **pénalisation L2** et le **early stopping** étaient couramment utilisées, mais elles ne suffisaient pas toujours à éviter l'adaptation excessive aux données d'entraînement.\n",
    "\n",
    "L'idée principale derrière le Dropout était inspirée de l'**apprentissage par ensembles**, où plusieurs modèles indépendants sont combinés pour améliorer la généralisation. Toutefois, entraîner et stocker plusieurs réseaux de neurones profonds était **coûteux en calcul**. Srivastava et son équipe ont alors cherché un moyen d'obtenir un effet similaire au **model averaging**, mais de manière bien **plus efficace**.\n",
    "\n",
    "\n",
    "### Fonctionnement du Dropout\n",
    "\n",
    "Leur solution a été de **perturber l'apprentissage en désactivant aléatoirement des neurones à chaque itération**, forçant ainsi chaque neurone à apprendre des représentations **plus robustes** sans dépendre excessivement de neurones spécifiques. En empêchant la formation de **co-adaptations trop spécialisées**, cette approche a conduit à des modèles généralisant mieux aux données non vues. De plus, lors de la phase de **test**, tous les **neurones sont activés** mais leurs **poids sont ajustés** pour compenser les désactivations précédentes, simulant ainsi un moyennage implicite d'un grand nombre de **réseaux plus petits entraînés en parallèle**.\n",
    "\n",
    "\n",
    "### Impact sur l’apprentissage des réseaux de neurones\n",
    "Le Dropout a plusieurs effets bénéfiques sur l'apprentissage :\n",
    "- **Réduction du surapprentissage** : en empêchant les neurones de trop s’adapter aux données d’entraînement.\n",
    "- **Amélioration de la robustesse** : chaque neurone doit apprendre des représentations plus générales, car il ne peut pas compter sur d’autres neurones spécifiques.\n",
    "- **Effet d’ensemble (ensemble learning)** : en échantillonnant différents sous-réseaux à chaque itération, le modèle final se comporte comme une combinaison de plusieurs réseaux différents, ce qui améliore la généralisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Exemple d'application de la technique du Dropout.\n",
    "\n",
    "### Présentation du modèle \n",
    "\n",
    "Ca sera plus simple pour tout le monde si je reprends un modèle que l'on connait. On va tout simplement utiliser le modèle que nous avions défini au TD de deep learning sur le jeu de données FASHION-MNIST. Nous partirons de là et nous appliquerons la technique du dropout pour vraiment comprendre l'interêt de cette technique.\n",
    "\n",
    "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) est un jeu de données contenant des images d'articles de Zalando, composé d'un ensemble d'entraînement de 60 000 exemples et d'un ensemble de test de 10 000 exemples. Chaque exemple est une image en niveaux de gris de 28x28 pixels, associée à une étiquette parmi 10 classes. \n",
    "<img src=\"img/fashion-mnist-small.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Charger le dataset Fashion-MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "trainset, validset = random_split(dataset, (50000, 10000))\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Définition du modèle sans Dropout\n",
    "class ReLUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instancier le modèle\n",
    "model = ReLUNet()\n",
    "\n",
    "# Définition de la fonction d'entraînement\n",
    "def train_model(model, trainloader, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")\n",
    "\n",
    "# Entraîner le modèle\n",
    "train_model(model, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle est fini nous allons maintenant le tester et vérifier son erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester le modèle et afficher l'erreur\n",
    "def test_model(model, validloader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validloader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"Test Loss: {total_loss/len(validloader):.4f}\")\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Exécuter le test\n",
    "test_model(model, validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Sans le dropout, le modèle a tendance à surapprendre, nous voyons alors que lors des tests l'erreur est très grande.\n",
    "***\n",
    "### Application du dropout\n",
    "\n",
    "Nous allons ici étapes par étapes appliquer le dropout à notre modèle. Les différentes étapes seront présentées sous forme de questions auquelles vous pourrez tenter de répondre. Il est fortement conseillé de tenter de résoudre les questions par vous même  pour comprendre le dropout en pratique et savoir l'appliquer.\n",
    "\n",
    "Torch utilise le dropout spatial, qui est décrit ici : https://arxiv.org/pdf/1411.4280.pdf\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 1 : Commence par définir un modèle de réseau de neurones simple avec une couche de dropout. Cette couche doit être insérée après une couche entièrement connectée. Rappelle-toi que la couche Dropout prend un paramètre p qui définit la probabilité de désactivation des neurones. Je te conseille pour commencer de poser p=0.5 (50% de chance d'être désactivée.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "# Fonction d'entraînement avec suivi de l'historique\n",
    "def train(model, trainloader, validloader, epochs=5, earlystopping=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_history, valid_history = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Mode entraînement (dropout activé)\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Suivi de l'exactitude de l'entraînement\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # Suivi de la perte et de l'exactitude de l'entraînement\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        train_history.append((train_loss, train_acc))\n",
    "\n",
    "        # Validation après chaque époque\n",
    "        model.eval()  # Mode évaluation (dropout désactivé)\n",
    "        valid_loss = 0.0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validloader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "        # Suivi de la perte et de l'exactitude de la validation\n",
    "        valid_loss = valid_loss / len(validloader)\n",
    "        valid_acc = 100 * correct_valid / total_valid\n",
    "        valid_history.append((valid_loss, valid_acc))\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Epoch {epoch+1} - Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_acc:.2f}%\")\n",
    "\n",
    "        if earlystopping:\n",
    "            # Implémentation possible pour early stopping\n",
    "            pass\n",
    "\n",
    "    return train_history, valid_history\n",
    "\n",
    "# Visualisation des courbes de perte et de précision\n",
    "def plot_train_val(train_history, valid_history):\n",
    "    train_loss, train_acc = zip(*train_history)\n",
    "    valid_loss, valid_acc = zip(*valid_history)\n",
    "\n",
    "    # Tracer la courbe de perte\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(valid_loss, label='Valid Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Tracer la courbe de précision\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.plot(valid_acc, label='Valid Accuracy')\n",
    "    plt.title('Accuracy per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/Q1.py\n",
    "class ModelWithDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        ...\n",
    "        # A compléter\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        # A compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle avec une couche de dropout\n",
    "net = ModelWithDropout()\n",
    "train_history, valid_history = train(net, earlystopping=False)\n",
    "plot_train_val(train_history, valid_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous avons ajouté une couche Dropout après la première couche linéaire. Cela permet de \"désactiver\" aléatoirement une partie des neurones pendant l'entraînement.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 2 : Maintenant, modifie ton modèle en ajoutant plusieurs couches de dropout dans le réseau. Chaque couche devrait avoir une probabilité différente de désactivation des neurones. Par exemple, la première couche de dropout pourrait avoir p=0.3 et la deuxième p=0.5. Essaie d'appliquer des dropouts à différentes étapes du réseau.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/Q2.py\n",
    "class RandomDropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        # A compléter\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        # A compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle avec dropouts aléatoires\n",
    "net = RandomDropoutModel()\n",
    "train_history, valid_history = train(net, trainloader, validloader, earlystopping=False)\n",
    "plot_train_val(train_history, valid_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce modèle, nous appliquons des dropouts après chaque couche linéaire. Le premier dropout est avec une probabilité de 30 % et le second de 50 %. \n",
    "\n",
    "Ces pourcentages ne sont pas selectionnés au hasard :\n",
    "- 30% de dropout dans les premières couches : garde une bonne quantité d'informations pour apprendre des représentations générales des données tout en réduisant le risque de surapprentissage.\n",
    "- 50% de dropout dans les couches profondes : favorise la généralisation en forçant le modèle à apprendre des représentations plus robustes et moins spécifiques aux données d'entraînement.\n",
    "\n",
    "Ainsi, la stratégie consiste à augmenter le dropout dans les couches profondes, où le modèle est déjà plus spécialisé, et à le garder modéré dans les couches initiales pour ne pas perdre des informations essentielles trop tôt.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 3 : Compare les performances avec le modèle initial.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du modèle avec Dropout\n",
    "class ReLUNetWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUNetWithDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout avec probabilité de 50%\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Appliquer Dropout après la première couche cachée\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instancier le modèle\n",
    "model = ReLUNetWithDropout()\n",
    "\n",
    "# Définition de la fonction d'entraînement\n",
    "def train_model(model, trainloader, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")\n",
    "\n",
    "# Entraîner le modèle\n",
    "train_model(model, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bravo, vous avez appliqué le dropout au modèle de reconnaissance de chiffres de MNIST. J'espère que ca s'est bien passé et que vous avez bien saisi comment est ce que le dropout intervient dans l'entrainement pour éviter le surapprentissage. \n",
    "\n",
    "Je me permets de te transmettre un petit message du créateur de ce notebook :\n",
    "\n",
    "*Salut à toi jeune codeur ! J'espère que ce petit message va te faire sourire au milieu de tes corrections de notebooks interminables. Je te souhaite bon courage pour la suite et surtout n'oublie pas de me mettre une bonne note, sinon je serais pas content. Bisous*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDD_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
