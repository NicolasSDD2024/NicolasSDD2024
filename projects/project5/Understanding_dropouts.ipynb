{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Le **surapprentissage** est un problème majeur dans l'entraînement des réseaux de neurones profonds. Une technique efficace pour le combattre est le **Dropout**.\n",
    "\n",
    "Dans ce notebook, nous allons :\n",
    "1. Mais le dropout, c'est quoi ?\n",
    "2. Exemple d'application de la technique du Dropout.\n",
    "3. Comparer les performances d'un réseau avec et sans Dropout.\n",
    "4. Présenter cette technique sur d'autres types de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: black; padding: 10px; border-radius: 5px;\">\n",
    "Plan d'action, on va faire un exemple de dropout et ensuite on va venir comparer aux autres techniques de régularisation et enfin on va venir les combiner et montrer que le dropout se combine très bien avec du L2 ou norma par lots par exemple dans notre cas (MNIST).\n",
    "\n",
    "Ensuite on va venir faire un état de l'art des différents réseaux de neuronnes qui sont efficaces avec le dropouts ou pas et conclure quant à cette technique et son utilisation sur les modèles de deep.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Mais le dropout, c'est quoi ?\n",
    "\n",
    "### Un peu d'histoire\n",
    "Avant l’introduction du Dropout, plusieurs approches avaient déjà exploré l’idée d’ajouter du bruit aux réseaux de neurones pour améliorer leur généralisation. **Hanson (1990)** a proposé la **Stochastic Delta Rule**, qui injectait du bruit dans l’apprentissage des poids pour limiter le surapprentissage. **Bishop (1995)** a démontré que **perturber les entrées** d’un modèle pouvait être interprété comme une forme de régularisation bayésienne. **LeCun et al. (1998)** ont testé l’**ajout de bruit** dans les activations des neurones pour limiter la dépendance excessive aux données d'entraînement. D’autres travaux, comme ceux de **Hinton & Nowlan (1992)** et **Neal (1995, 2001)**, ont étudié l’utilisation de **distributions probabilistes sur les poids et les activations** afin d’améliorer la généralisation des modèles.\n",
    "\n",
    "Le **Dropout** a été introduit en **2014** par **Srivastava et al.** dans leur article **\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"**. Cette technique a été développée pour pallier le problème du surapprentissage dans les réseaux de neurones profonds. Avant son introduction, les méthodes classiques de régularisation comme la **pénalisation L2** et le **early stopping** étaient couramment utilisées, mais elles ne suffisaient pas toujours à éviter l'adaptation excessive aux données d'entraînement.\n",
    "\n",
    "L'idée principale derrière le Dropout était inspirée de l'**apprentissage par ensembles**, où plusieurs modèles indépendants sont combinés pour améliorer la généralisation. Toutefois, entraîner et stocker plusieurs réseaux de neurones profonds était **coûteux en calcul**. Srivastava et son équipe ont alors cherché un moyen d'obtenir un effet similaire au **model averaging**, mais de manière bien **plus efficace**.\n",
    "\n",
    "\n",
    "### Fonctionnement du Dropout\n",
    "\n",
    "Leur solution a été de **perturber l'apprentissage en désactivant aléatoirement des neurones à chaque itération**, forçant ainsi chaque neurone à apprendre des représentations **plus robustes** sans dépendre excessivement de neurones spécifiques. En empêchant la formation de **co-adaptations trop spécialisées**, cette approche a conduit à des modèles généralisant mieux aux données non vues. De plus, lors de la phase de **test**, tous les **neurones sont activés** mais leurs **poids sont ajustés** pour compenser les désactivations précédentes, simulant ainsi un moyennage implicite d'un grand nombre de **réseaux plus petits entraînés en parallèle**.\n",
    "\n",
    "\n",
    "### Impact sur l’apprentissage des réseaux de neurones\n",
    "Le Dropout a plusieurs effets bénéfiques sur l'apprentissage :\n",
    "- **Réduction du surapprentissage** : en empêchant les neurones de trop s’adapter aux données d’entraînement.\n",
    "- **Amélioration de la robustesse** : chaque neurone doit apprendre des représentations plus générales, car il ne peut pas compter sur d’autres neurones spécifiques.\n",
    "- **Effet d’ensemble (ensemble learning)** : en échantillonnant différents sous-réseaux à chaque itération, le modèle final se comporte comme une combinaison de plusieurs réseaux différents, ce qui améliore la généralisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Exemple d'application de la technique du Dropout.\n",
    "\n",
    "### Présentation du modèle \n",
    "\n",
    "Ca sera plus simple pour tout le monde si je reprends un modèle que l'on connait. On va tout simplement utiliser le modèle que nous avions défini au TD de deep learning sur le jeu de données FASHION-MNIST. Nous partirons de là et nous appliquerons la technique du dropout pour vraiment comprendre l'interêt de cette technique.\n",
    "\n",
    "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) est un jeu de données contenant des images d'articles de Zalando, composé d'un ensemble d'entraînement de 60 000 exemples et d'un ensemble de test de 10 000 exemples. Chaque exemple est une image en niveaux de gris de 28x28 pixels, associée à une étiquette parmi 10 classes. \n",
    "\n",
    "<img src=\"img/fashion-mnist-small.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction d'entraînement avec suivi de l'historique\n",
    "def train(model, trainloader, validloader, epochs=5, lr=0.001, earlystopping=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_history, valid_history = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Mode entraînement (dropout activé)\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Suivi de l'exactitude de l'entraînement\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # Suivi de la perte et de l'exactitude de l'entraînement\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        train_history.append((train_loss, train_acc))\n",
    "\n",
    "        # Validation après chaque époque\n",
    "        model.eval()  # Mode évaluation (dropout désactivé)\n",
    "        valid_loss = 0.0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validloader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "        # Suivi de la perte et de l'exactitude de la validation\n",
    "        valid_loss = valid_loss / len(validloader)\n",
    "        valid_acc = 100 * correct_valid / total_valid\n",
    "        valid_history.append((valid_loss, valid_acc))\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Epoch {epoch+1} - Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_acc:.2f}%\")\n",
    "\n",
    "        if earlystopping:\n",
    "            # Implémentation possible pour early stopping\n",
    "            pass\n",
    "\n",
    "    return train_history, valid_history\n",
    "\n",
    "# Visualisation des courbes de perte et de précision\n",
    "def plot_train_val(train_history, valid_history):\n",
    "    train_loss, train_acc = zip(*train_history)\n",
    "    valid_loss, valid_acc = zip(*valid_history)\n",
    "\n",
    "    # Tracer la courbe de perte\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(valid_loss, label='Valid Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Tracer la courbe de précision\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.plot(valid_acc, label='Valid Accuracy')\n",
    "    plt.title('Accuracy per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset Fashion-MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "trainset, validset = random_split(dataset, (50000, 10000))\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous choisissons de définir un batch de taille 64...\n",
    "\n",
    "Nous allons ensuite définir la classe du réseau de neurones qui n'utilise pas de dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Définition du modèle sans Dropout\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReLUNet\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(ReLUNet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Définition du modèle sans Dropout\n",
    "class ReLUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        self.fc2 = nn.Linear(120,60)\n",
    "        self.fc3 = nn.Linear(60, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe est définie, nous allons maintenant instancier un nouveau réseau, le tester et vérifier son erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciation et entraînement du modèle sans dropout\n",
    "model = ReLUNet()\n",
    "net_no_dropout = ReLUNet()  # Créer le modèle sans dropout\n",
    "train_history_no_dropout, valid_history_no_dropout = train(\n",
    "    net_no_dropout, \n",
    "    trainloader, validloader, \n",
    "    epochs=5, \n",
    "    lr=0.001\n",
    "    )\n",
    "plot_train_val(train_history_no_dropout, valid_history_no_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Sans le dropout, le modèle a tendance à surapprendre, nous voyons alors que lors de la validation, l'erreur a tendance à grandir.\n",
    "***\n",
    "### Application du dropout\n",
    "\n",
    "Nous allons ici étapes par étapes appliquer le dropout à notre modèle. Les différentes étapes seront présentées sous forme de questions auquelles vous pourrez tenter de répondre. Il est fortement conseillé de tenter de résoudre les questions par vous même  pour comprendre le dropout en pratique et savoir l'appliquer.\n",
    "\n",
    "Torch utilise le dropout spatial, qui est décrit ici : https://arxiv.org/pdf/1411.4280.pdf\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 1 : Commence par définir un modèle de réseau de neurones simple avec une couche de dropout. Cette couche doit être insérée après une couche entièrement connectée. Rappelle-toi que la couche Dropout prend un paramètre p qui définit la probabilité de désactivation des neurones. Je te conseille pour commencer de poser p=0.5 (50% de chance d'être désactivée.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/Q1.py\n",
    "class ModelWithSingleDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithSingleDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        \n",
    "        ... # Créer la couche de dropout avec p=0.5\n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        ... # Appliquer le dropout après la première couche\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction\n",
    "\n",
    "class ModelWithSingleDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithSingleDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # créer la couche de dropout avec p=0.5\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Appliquer le dropout après la première couche\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous avons ajouté une couche Dropout après la première couche linéaire. Cela permet de \"désactiver\" aléatoirement une partie des neurones de cette couche pendant l'entraînement.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 2 : Maintenant, modifie ton modèle en ajoutant plusieurs couches de dropout dans le réseau. Chaque couche devrait avoir une probabilité différente de désactivation des neurones. Par exemple, la première couche de dropout pourrait avoir p=0.3 et la deuxième p=0.5. Essaie d'appliquer des dropouts à différentes étapes du réseau.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/Q2.py\n",
    "class RandomDropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        # A compléter\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        # A compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Définition du modèle avec Dropout\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRandomDropoutModel\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(RandomDropoutModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# correction\n",
    "\n",
    "# Définition de la classe du modèle avec Dropout\n",
    "class RandomDropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RandomDropoutModel, self).__init__()\n",
    "        self.dropout1 = nn.Dropout(p=0.2) # Dropout avec une probabilité de 20%\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)  # Dropout avec une probabilité de 50%\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)  # Dropout avec une probabilité de 50%\n",
    "        self.fc3 = nn.Linear(60, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.dropout1(x)  # Appliquer le premier dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)  # Appliquer le deuxième dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout3(x)  # Appliquer le troisième dropout\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation et entraînement du modèle avec dropouts aléatoires\n",
    "net = RandomDropoutModel()\n",
    "train_history_with_dropout, valid_history_with_dropout = train(\n",
    "    net, \n",
    "    trainloader, validloader, \n",
    "    epochs=5,\n",
    "    lr=0.001,\n",
    "    earlystopping=False)\n",
    "plot_train_val(train_history_with_dropout, valid_history_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant à toi de jouer je te laisse créer un nouveau modèle de dropout avec les pourcentages que tu souhaites, essaye avec des petites et grandes valeurs pour bien visualiser l'effet du dropout.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 3 : Teste d'autres valeurs \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de ton modèle avec Dropout\n",
    "class YourDropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YourDropoutModel, self).__init__()\n",
    "        self.dropout1 = nn.Dropout(p=...)\n",
    "        self.fc1 = nn.Linear(784, 120)\n",
    "        self.dropout2 = nn.Dropout(p=...)\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.dropout3 = nn.Dropout(p=...)\n",
    "        self.fc3 = nn.Linear(60, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.dropout1(x)  # Appliquer le premier dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)  # Appliquer le deuxième dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout3(x)  # Appliquer le troisième dropout\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation et entraînement de ton modèle avec dropouts aléatoires\n",
    "yournet = YourDropoutModel()\n",
    "train_history_your_dropout, valid_history_your_dropout = train(\n",
    "    yournet, \n",
    "    trainloader, validloader, \n",
    "    epochs=5,\n",
    "    lr=..., # Ici tu peux le modifier pour visualiser son effet, \n",
    "# il est conseillé de l'augmenter légèrement par rapport au modèle sans dropout pour contrer le bruit généré\n",
    "    earlystopping=False)\n",
    "plot_train_val(train_history_your_dropout, valid_history_your_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous comprennez maintenant l'interêt de fixer des pourcentages précis :\n",
    "- 20% de dropout dans les premières couches : garde une bonne quantité d'informations pour apprendre des représentations générales des données tout en réduisant le risque de surapprentissage.\n",
    "- 50% de dropout dans les couches profondes : favorise la généralisation en forçant le modèle à apprendre des représentations plus robustes et moins spécifiques aux données d'entraînement.\n",
    "\n",
    "Ainsi, la stratégie consiste à augmenter le dropout dans les couches profondes, où le modèle est déjà plus spécialisé, et à le garder modéré dans les couches initiales pour ne pas perdre des informations essentielles trop tôt.\n",
    "\n",
    "### Bravo ! \n",
    "Vous avez appliqué le dropout au modèle de reconnaissance de chiffres de MNIST. J'espère que ca s'est bien passé et que vous avez bien saisi comment est ce que le dropout intervient dans l'entrainement pour éviter le surapprentissage. \n",
    "\n",
    "Je me permets de te transmettre un petit message du créateur de ce notebook :\n",
    "\n",
    "*Salut à toi jeune codeur ! J'espère que ce petit message va te faire sourire au milieu de tes corrections de notebooks interminables. Je te souhaite bon courage pour la suite et surtout n'oublie pas de me mettre une bonne note, sinon je serais pas content. Bisous*\n",
    "***\n",
    "\n",
    "## 3. Comparer les performances d'un réseau avec et sans Dropout.\n",
    "\n",
    "Le code suivant permet de comparer les performances des modèles précédents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la figure avec les 2 résultats précédents\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Tracer la courbe de perte (Loss)\n",
    "axs[0].plot([x[0] for x in train_history_no_dropout], label='Train Loss (No Dropout)', color='blue')\n",
    "axs[0].plot([x[0] for x in valid_history_no_dropout], label='Valid Loss (No Dropout)', color='red')\n",
    "axs[0].plot([x[0] for x in train_history_with_dropout], label='Train Loss (With Dropout)', color='green')\n",
    "axs[0].plot([x[0] for x in valid_history_with_dropout], label='Valid Loss (With Dropout)', color='orange')\n",
    "axs[0].plot([x[0] for x in train_history_your_dropout], label='Train Loss (Your Model)', color='yellow')\n",
    "axs[0].plot([x[0] for x in valid_history_your_dropout], label='Valid Loss (Your Model)', color='pink')\n",
    "axs[0].set_title('Perte (Loss) par époque')\n",
    "axs[0].set_xlabel('Époques')\n",
    "axs[0].set_ylabel('Perte')\n",
    "axs[0].legend()\n",
    "\n",
    "# Tracer la courbe de précision (Accuracy)\n",
    "axs[1].plot([x[1] for x in train_history_no_dropout], label='Train Accuracy (No Dropout)', color='blue')\n",
    "axs[1].plot([x[1] for x in valid_history_no_dropout], label='Valid Accuracy (No Dropout)', color='red')\n",
    "axs[1].plot([x[1] for x in train_history_with_dropout], label='Train Accuracy (With Dropout)', color='green')\n",
    "axs[1].plot([x[1] for x in valid_history_with_dropout], label='Valid Accuracy (With Dropout)', color='orange')\n",
    "axs[1].plot([x[1] for x in train_history_your_dropout], label='Train Accuracy (Your Model)', color='yellow')\n",
    "axs[1].plot([x[1] for x in valid_history_your_dropout], label='Valid Accuracy (Your Model)', color='pink')\n",
    "axs[1].set_title('Précision (Accuracy) par époque')\n",
    "axs[1].set_xlabel('Époques')\n",
    "axs[1].set_ylabel('Précision (%)')\n",
    "axs[1].legend()\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La il faut comparer les deux etc .....\n",
    "\n",
    "Lorsqu'on va encore plus loin dans les epochs, l'effet des droppouts est encore plus visible, les temps de calculs sont longs donc je vais pas vous les faire faire mais voici les résultats que j'ai obtenu précédemment avec 50 epochs :\n",
    "\n",
    "<img src= \"img/resultats - p1=0,2 - p2=0,5 - p3=0,5 - 50 epochs.png\" style=\"width: 900px; height: auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Présenter cette technique sur d'autres types de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDD_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
