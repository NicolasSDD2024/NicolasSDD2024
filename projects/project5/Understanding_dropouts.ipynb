{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Le **surapprentissage** est un problème majeur dans l'entraînement des réseaux de neurones profonds. Une technique efficace pour le combattre est le **Dropout**.\n",
    "\n",
    "Dans ce notebook, nous allons :\n",
    "1. Mais le dropout, c'est quoi ?\n",
    "2. Exemple d'application de la technique du Dropout.\n",
    "3. Comparer les performances d'un réseau avec et sans Dropout.\n",
    "4. Présenter cette technique sur d'autres types de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: black; padding: 10px; border-radius: 5px;\">\n",
    "Plan d'action, on va faire un exemple de dropout et ensuite on va venir comparer aux autres techniques de régularisation et enfin on va venir les combiner et montrer que le dropout se combine très bien avec du L2 ou norma par lots par exemple dans notre cas (MNIST).\n",
    "\n",
    "Ensuite on va venir faire un état de l'art des différents réseaux de neuronnes qui sont efficaces avec le dropouts ou pas et conclure quant à cette technique et son utilisation sur les modèles de deep.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Mais le dropout, c'est quoi ?\n",
    "\n",
    "### Un peu d'histoire\n",
    "Avant l’introduction du Dropout, plusieurs approches avaient déjà exploré l’idée d’ajouter du bruit aux réseaux de neurones pour améliorer leur généralisation. **Hanson (1990)** a proposé la **Stochastic Delta Rule**, qui injectait du bruit dans l’apprentissage des poids pour limiter le surapprentissage. **Bishop (1995)** a démontré que **perturber les entrées** d’un modèle pouvait être interprété comme une forme de régularisation bayésienne. **LeCun et al. (1998)** ont testé l’**ajout de bruit** dans les activations des neurones pour limiter la dépendance excessive aux données d'entraînement. D’autres travaux, comme ceux de **Hinton & Nowlan (1992)** et **Neal (1995, 2001)**, ont étudié l’utilisation de **distributions probabilistes sur les poids et les activations** afin d’améliorer la généralisation des modèles.\n",
    "\n",
    "Le **Dropout** a été introduit en **2014** par **Srivastava et al.** dans leur article **\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"**. Cette technique a été développée pour pallier le problème du surapprentissage dans les réseaux de neurones profonds. Avant son introduction, les méthodes classiques de régularisation comme la **pénalisation L2** et le **early stopping** étaient couramment utilisées, mais elles ne suffisaient pas toujours à éviter l'adaptation excessive aux données d'entraînement.\n",
    "\n",
    "L'idée principale derrière le Dropout était inspirée de l'**apprentissage par ensembles**, où plusieurs modèles indépendants sont combinés pour améliorer la généralisation. Toutefois, entraîner et stocker plusieurs réseaux de neurones profonds était **coûteux en calcul**. Srivastava et son équipe ont alors cherché un moyen d'obtenir un effet similaire au **model averaging**, mais de manière bien **plus efficace**.\n",
    "\n",
    "\n",
    "### Fonctionnement du Dropout\n",
    "\n",
    "Leur solution a été de **perturber l'apprentissage en désactivant aléatoirement des neurones à chaque itération**, forçant ainsi chaque neurone à apprendre des représentations **plus robustes** sans dépendre excessivement de neurones spécifiques. En empêchant la formation de **co-adaptations trop spécialisées**, cette approche a conduit à des modèles généralisant mieux aux données non vues. De plus, lors de la phase de **test**, tous les **neurones sont activés** mais leurs **poids sont ajustés** pour compenser les désactivations précédentes, simulant ainsi un moyennage implicite d'un grand nombre de **réseaux plus petits entraînés en parallèle**.\n",
    "\n",
    "\n",
    "### Impact sur l’apprentissage des réseaux de neurones\n",
    "Le Dropout a plusieurs effets bénéfiques sur l'apprentissage :\n",
    "- **Réduction du surapprentissage** : en empêchant les neurones de trop s’adapter aux données d’entraînement.\n",
    "- **Amélioration de la robustesse** : chaque neurone doit apprendre des représentations plus générales, car il ne peut pas compter sur d’autres neurones spécifiques.\n",
    "- **Effet d’ensemble (ensemble learning)** : en échantillonnant différents sous-réseaux à chaque itération, le modèle final se comporte comme une combinaison de plusieurs réseaux différents, ce qui améliore la généralisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Exemple d'application de la technique du Dropout.\n",
    "\n",
    "### Présentation du modèle \n",
    "\n",
    "Ca sera plus simple pour tout le monde si je reprends un modèle que l'on connait. On va tout simplement utiliser le modèle que nous avions défini au TD de deep learning sur la reconnaissance de chiffres manuscrits à l'aide de MNIST. Nous partirons de là et nous appliquerons la technique du dropout pour vraiment comprendre l'interêt de cette technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation des librairies, données et visualisation des différents datasets\n",
    "# création du modèle de réseaux de neuronnes\n",
    "# On fit le modèle \n",
    "# On fait des essais d'overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle est fini nous allons maintenant le tester et vérifier son erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du modèle \n",
    "# présentation des résultats sans le dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Sans le dropout, le modèle a tendance à surapprendre, nous voyons alors que lors des tests l'erreur est très grande.\n",
    "***\n",
    "### Application du dropout\n",
    "\n",
    "Nous allons ici étapes par étapes appliquer le dropout à notre modèle. Les différentes étapes seront présentées sous forme de questions auquelles vous pourrez tenter de répondre. Il est fortement conseillé de tenter de résoudre les questions par vous même  pour comprendre le dropout en pratique et savoir l'appliquer.\n",
    "\n",
    "Torch utilise le dropout spatial, qui est décrit ici : https://arxiv.org/pdf/1411.4280.pdf\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question 1 : Crée un réseau qui inclut au moins une couche de dropout.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/Q1.py\n",
    "class DropoutNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        ...\n",
    "        # A compléter\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        # A compléter\n",
    "\n",
    "net = DropoutNet()\n",
    "train_history, valid_history = train(net, earlystopping=False)\n",
    "plot_train_val(train_history, valid_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Question 2 : Crée un réseau qui inclut des dropouts aléatoirement.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Question 3 : Crée un code pour faire en sorte que le modèle s'entraine de manière répétée en appliquant à chaque entrainement, un dropout aléatoire.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Question 4 : Crée un code qui vient ajuster les poids des neurones et qui vient tester votre modèle.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bravo, vous avez appliqué le dropout au modèle de reconnaissance de chiffres de MNIST. J'espère que ca s'est bien passé et que vous avez bien saisi comment est ce que le dropout intervient dans l'entrainement pour éviter le surapprentissage. \n",
    "\n",
    "Je me permets de te transmettre un petit message du créateur de ce notebook :\n",
    "\n",
    "*Salut à toi jeune codeur ! J'espère que ce petit message va te faire sourire au milieu de tes corrections de notebooks interminables. Je te souhaite bon courage pour la suite et surtout n'oublie pas de me mettre une bonne note, sinon je serais pas content. Bisous*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importation des bibliothèques\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# C'est chat gpt qui me l'a généré\n",
    "\n",
    "# Importation des bibliothèques\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chargement des données MNIST\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalisation\n",
    "\n",
    "# Création d'un modèle simple sans Dropout\n",
    "def create_model(dropout_rate=0.0):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(512, activation='relu'),\n",
    "        keras.layers.Dropout(dropout_rate),  # Ajout du Dropout ici\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Entraînement du modèle sans Dropout\n",
    "model_no_dropout = create_model(dropout_rate=0.0)\n",
    "history_no_dropout = model_no_dropout.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Entraînement du modèle avec Dropout\n",
    "model_with_dropout = create_model(dropout_rate=0.5)\n",
    "history_with_dropout = model_with_dropout.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Comparaison des performances\n",
    "plt.plot(history_no_dropout.history['val_accuracy'], label='Sans Dropout')\n",
    "plt.plot(history_with_dropout.history['val_accuracy'], label='Avec Dropout')\n",
    "plt.xlabel('Épochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "## Conclusion\n",
    "Nous avons vu que le Dropout permet de réduire le surapprentissage et d'améliorer la généralisation du modèle.\n",
    "Des extensions possibles incluent l'expérimentation avec différents taux de Dropout ou l'application à d'autres modèles.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison avec d’autres techniques de régularisation\n",
    "Le Dropout est souvent comparé à d’autres techniques de régularisation utilisées pour améliorer la généralisation des modèles :\n",
    "- **L2 (Weight Decay)** : Ajoute une pénalité sur la taille des poids pour éviter des valeurs extrêmes.\n",
    "- **L1 (Lasso)** : Encourage la parcimonie en poussant certains poids vers zéro.\n",
    "- **Max-norm** : Contraint les poids à ne pas dépasser une certaine norme.\n",
    "- **Batch Normalization** : Normalise les activations pour stabiliser l’apprentissage et accélérer la convergence.\n",
    "\n",
    "Chacune de ces méthodes a ses avantages, mais le Dropout est particulièrement efficace lorsqu'il est combiné avec d’autres régularisations, comme L2 ou la normalisation par lots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDD_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
